{# Copyright (c) Microsoft Corporation. #}
{# Licensed under the MIT license. #}

{% set IDX_T = "long long" if BCSR and BCSC else "int" %}

__device__ __forceinline__ const float* add_ptr_f(const float* src, int offset) \
{                                                                               \
    const float* dst;                                                           \
    asm("{                       \n\t"                                          \
        ".reg .u32 lo,hi,of;     \n\t"                                          \
        "mul.lo.u32 of, %2, %3;  \n\t"                                          \
        "mov.b64    {lo,hi}, %1; \n\t"                                          \
        "add.cc.u32  lo,lo,  of; \n\t"                                          \
        "addc.u32    hi,hi,  0;  \n\t"                                          \
        "mov.b64 %0, {lo,hi};    \n\t"                                          \
        "}" : "=l"(dst) : "l"(src), "r"(offset), "r"((int)sizeof(*src)));       \
    return dst;                                                                 \
}

__device__ __forceinline__ float2 _add(float2 x, float2 y) \
{                                                          \
    float2 res;                                            \
    res.x = x.x + y.x;                                     \
    res.y = x.y + y.y;                                     \
    return res;                                            \
}

__global__ void BLOCK_SPARSE_MATMUL_OUT_32_64_32(
    float* A,
    float* B,
    {% if BIASED %}float* bias,{% endif %}
    float* C_val,
    {{ IDX_T }}* C_block_idx,
    int C_block_nnz,
    int M,
    int K,
    int N
) {
    /*
    description:
    tiling k dimension
    tile size: 32x64x32
    smm_dd_s_nn: sparse matmul, dense (MxK, along K) x dense (NxK, along K) -> sparse (MxN, along N)
    smm_dd_s_nt: sparse matmul, dense (MxK, along K) x dense (KxN, along N, need TRANSPOSE_B) -> sparse (MxN, along N)
    smm_dd_s_tn: sparse matmul, dense (KxM, along M, need TRANSPOSE_A) x dense (NxK, along K) -> sparse (MxN, along N)
    smm_dd_s_tt: sparse matmul, dense (KxM, along M, need TRANSPOSE_A) x dense (KxN, along N, need TRANSPOSE_B) -> sparse (MxN, along N)
    the output sparse is block size 32x32, the blocks will be written to bcsr 32x64
    Known issue: the output 
    */
    const int BLOCK_SIZE_M = 32;  // 64
    const int BLOCK_SIZE_N = 32;  //128
    const int THREAD_SIZE_K = 64;

    {% if BATCHED %}
    A += M*K*blockIdx.y;
    B += K*N*blockIdx.y;
    {% if COMPRESSED %}
    C_val += C_block_nnz*32*32*blockIdx.y;
    {% else %}
    C_val += M*N*blockIdx.y;
    {% endif %}
    {% if BIASED %}
    bias += N*blockIdx.y;
    {% endif %}
    {% endif %}

    assert(blockDim.x % 32 == 0);
    uint n_warp = 8; // blockDim.x / 32
    assert(THREAD_SIZE_K % n_warp == 0);
    // THREAD_SIZE_K: one loop k
    assert(K % THREAD_SIZE_K == 0);

    assert(BLOCK_SIZE_M == BLOCK_SIZE_N);
    __shared__ float fShare[65 * 32 * 2];
    char* bShare = (char*)fShare;
    {% if BIASED %}__shared__ float bias_share[BLOCK_SIZE_N];{% endif %}

    uint tid = threadIdx.x;
    {{ IDX_T }} idx = C_block_idx[blockIdx.x];
    {% if BCSR %}
        {% if BCSC %}
    uint blk_index = idx >> 32;
    idx &= 0xffffffff;
        {% else %}
    uint blk_index = blockIdx.x;
        {% endif %}
    uint bx = idx & 0xffff;
    uint by = idx >> 16;
    {% else %}
    uint blk_index = blockIdx.x;
    uint bx = idx >> 16;
    uint by = idx & 0xffff;
    {% endif %}
    {% if BIASED %}
    if (tid < BLOCK_SIZE_N) {
        bias_share[tid] = bias[bx * BLOCK_SIZE_N + tid % 32]; 
    }
    {% endif %}
    uint tx = tid % 16;
    uint ty = tid / 16;
    assert(THREAD_SIZE_K % 16 == 0);

    // A is stored in dense format
    {% if TRANSPOSE_A %}
    uint ori_offsetA00 = tid / (BLOCK_SIZE_M/4) * M + by * BLOCK_SIZE_M + (tid % (BLOCK_SIZE_M/4)) * 4;
    uint ori_offsetA16 = ori_offsetA00 + M * 32;
    uint storA = (tid * 4 + tid / (BLOCK_SIZE_M/4) / 4 * 2) * 4;
    {% else %}
    uint ori_offsetA00 = (by * 32 + ty) * K + tx * 4;
    uint ori_offsetA16 = ori_offsetA00 + K * 16;
    {% endif %}

    // B is stored in dense format
    {% if TRANSPOSE_B %}
    uint ori_offsetB00 = (bx * 32 + ty) * K + tx * 4;
    uint ori_offsetB16 = ori_offsetB00 + K * 16;
    {% else %}
    uint ori_offsetB00 = tid / (BLOCK_SIZE_N/4) * N + bx * BLOCK_SIZE_N + (tid % (BLOCK_SIZE_N/4)) * 4;
    uint ori_offsetB16 = ori_offsetB00 + N * 32;
    uint storB = (tid * 4 + tid / (BLOCK_SIZE_N/4) / 4 * 2) * 4;
    {% endif %}
    
    {% if not COMPRESSED %}
    C_val += (by * 32 + ty) * N + (bx * 32 + tx * 2);
    {% endif %}

    uint tid224 = tid & 224;
    uint storAB = (tx * 32 * 4 + ty + tx * 2) * 4;
    uint loadA = (((tid & 16) >> 3) | (tid & 1)) << 4;
    uint loadB = ((tid >> 1) & 7) << 4;
    loadA += (tid224 * 32) + (tid224 / 2);
    loadB += (tid224 * 32) + (tid224 / 2);

    // This keeps all prior logic outside of the loops.
    asm("mov.b32 %0, %0;" : "+r"(storAB) : );
    asm("mov.b32 %0, %0;" : "+r"(loadA)  : );
    asm("mov.b32 %0, %0;" : "+r"(loadB)  : );

    float regC[8][4];
    for (int i = 0; i < 8; i++)
        for (int j = 0; j < 4; j++)
            regC[i][j] = 0.0f;

    for (int k_seq = 0; k_seq < (int)(K/64); k_seq++)
    {
        {% if TRANSPOSE_A %}
        uint offsetA00 = ori_offsetA00 + 64 * k_seq * M;
        uint offsetA16 = ori_offsetA16 + 64 * k_seq * M;
        {% else %}
        uint offsetA00 = ori_offsetA00 + 64 * k_seq;
        uint offsetA16 = ori_offsetA16 + 64 * k_seq;
        {% endif %}
        {% if TRANSPOSE_B %}
        uint offsetB00 = ori_offsetB00 + 64 * k_seq;
        uint offsetB16 = ori_offsetB16 + 64 * k_seq;
        {% else %}
        uint offsetB00 = ori_offsetB00 + 64 * k_seq * N;
        uint offsetB16 = ori_offsetB16 + 64 * k_seq * N;
        {% endif %}

        float4 a00 = {0}, a16 = {0};
        float4 b00 = {0}, b16 = {0};
        a00 = __ldg((const float4*)(add_ptr_f(A, offsetA00)));
        a16 = __ldg((const float4*)(add_ptr_f(A, offsetA16)));
        b00 = __ldg((const float4*)(add_ptr_f(B, offsetB00)));
        b16 = __ldg((const float4*)(add_ptr_f(B, offsetB16)));

        __syncthreads();

        {% if TRANSPOSE_A %}
        *(float*)&bShare[storA + (0*65*32 + 0)*4] = a00.x;
        *(float*)&bShare[storA + (0*65*32 + 1)*4] = a00.y;
        *(float*)&bShare[storA + (0*65*32 + 2)*4] = a00.z;
        *(float*)&bShare[storA + (0*65*32 + 3)*4] = a00.w;
        *(float*)&bShare[storA + (32*32 + 8*2 + 0*65*32 + 0)*4] = a16.x;
        *(float*)&bShare[storA + (32*32 + 8*2 + 0*65*32 + 1)*4] = a16.y;
        *(float*)&bShare[storA + (32*32 + 8*2 + 0*65*32 + 2)*4] = a16.z;
        *(float*)&bShare[storA + (32*32 + 8*2 + 0*65*32 + 3)*4] = a16.w;
        {% else %}
        *(float*)&bShare[storAB + (0*32 +  0 + 0*65*32)*4] = a00.x;
        *(float*)&bShare[storAB + (1*32 +  0 + 0*65*32)*4] = a00.y;
        *(float*)&bShare[storAB + (2*32 +  0 + 0*65*32)*4] = a00.z;
        *(float*)&bShare[storAB + (3*32 +  0 + 0*65*32)*4] = a00.w;
        *(float*)&bShare[storAB + (0*32 + 16 + 0*65*32)*4] = a16.x;
        *(float*)&bShare[storAB + (1*32 + 16 + 0*65*32)*4] = a16.y;
        *(float*)&bShare[storAB + (2*32 + 16 + 0*65*32)*4] = a16.z;
        *(float*)&bShare[storAB + (3*32 + 16 + 0*65*32)*4] = a16.w;
        {% endif %}

        {% if TRANSPOSE_B %}
        *(float*)&bShare[storAB + (0*32 +  0 + 1*65*32)*4] = b00.x;
        *(float*)&bShare[storAB + (1*32 +  0 + 1*65*32)*4] = b00.y;
        *(float*)&bShare[storAB + (2*32 +  0 + 1*65*32)*4] = b00.z;
        *(float*)&bShare[storAB + (3*32 +  0 + 1*65*32)*4] = b00.w;
        *(float*)&bShare[storAB + (0*32 + 16 + 1*65*32)*4] = b16.x;
        *(float*)&bShare[storAB + (1*32 + 16 + 1*65*32)*4] = b16.y;
        *(float*)&bShare[storAB + (2*32 + 16 + 1*65*32)*4] = b16.z;
        *(float*)&bShare[storAB + (3*32 + 16 + 1*65*32)*4] = b16.w;
        {% else %}
        *(float*)&bShare[storB + (1*65*32 + 0)*4] = b00.x;
        *(float*)&bShare[storB + (1*65*32 + 1)*4] = b00.y;
        *(float*)&bShare[storB + (1*65*32 + 2)*4] = b00.z;
        *(float*)&bShare[storB + (1*65*32 + 3)*4] = b00.w;
        *(float*)&bShare[storB + (32*32 + 8*2 + 1*65*32 + 0)*4] = b16.x;
        *(float*)&bShare[storB + (32*32 + 8*2 + 1*65*32 + 1)*4] = b16.y;
        *(float*)&bShare[storB + (32*32 + 8*2 + 1*65*32 + 2)*4] = b16.z;
        *(float*)&bShare[storB + (32*32 + 8*2 + 1*65*32 + 3)*4] = b16.w;
        {% endif %}

        __syncthreads();

        float regA[8], regB[4];
        #pragma unroll
        for (int j = 0; j < 4; j++)
        {
            // fetch outer product data
            *(float4*)&regA[0] = *(float4*)&bShare[loadA + (32*j +  0)*4];
            *(float4*)&regA[4] = *(float4*)&bShare[loadA + (32*j + 16)*4];
            *(float4*)&regB[0] = *(float4*)&bShare[loadB + (32*j + 65*32)*4];

            for (int i = 0; i < 8; i++)
                for (int j = 0; j < 4; j++)
                    regC[i][j] += regA[i] * regB[j];
        }
        #pragma unroll
        for (int j = 4; j < 8; j++)
        {
            *(float2*)&regA[0] = *(float2*)&bShare[loadA + (32*j +  0 + (j/4)*2)*4];
            *(float2*)&regA[2] = *(float2*)&bShare[loadA + (32*j +  2 + (j/4)*2)*4];
            *(float2*)&regA[4] = *(float2*)&bShare[loadA + (32*j + 16 + (j/4)*2)*4];
            *(float2*)&regA[6] = *(float2*)&bShare[loadA + (32*j + 18 + (j/4)*2)*4];
            *(float2*)&regB[0] = *(float2*)&bShare[loadB + (32*j +  0 + (j/4)*2 + 65*32)*4];
            *(float2*)&regB[2] = *(float2*)&bShare[loadB + (32*j +  2 + (j/4)*2 + 65*32)*4];

            for (int i = 0; i < 8; i++)
                for (int j = 0; j < 4; j++)
                    regC[i][j] += regA[i] * regB[j];
        }
    }

    asm volatile ("mov.u32 %0, %tid.x;"   : "=r"(tid)   :);
    asm volatile ("mov.u32 %0, %ctaid.x;" : "=r"(bx)   :);
    asm volatile ("mov.u32 %0, %ctaid.y;" : "=r"(by) :);

    ty = ((tid & 16) >> 3) + (tid & 1);
    tx = ((tid >> 1) & 7) + ((tid & 224) >> 2) + (ty << 2);

    uint storC = ty*32*8*4 + tx*4;

    tx = tid % 16;
    ty = tid / 16;

    uint readC = ty*32*8 + tx*2 + ((tid & 192)>>2);
    {% if COMPRESSED %}
    C_val += 32 * 32 * blk_index;
    C_val += ty * 32 + tx * 2;
    {% endif %}

    __syncthreads();
    *(float4*)&fShare[storC + 0*32*8] = *(float4*)regC[0];
    *(float4*)&fShare[storC + 1*32*8] = *(float4*)regC[1];
    *(float4*)&fShare[storC + 2*32*8] = *(float4*)regC[2];
    *(float4*)&fShare[storC + 3*32*8] = *(float4*)regC[3];
    __syncthreads();

    float2 c2[8];
    for (int i = 0; i < 8; i++)
        c2[i] = *(float2*)&fShare[readC + i*32];

    // Tree reduce
    for (int j = 4; j > 0; j >>= 1)
        for (int i = 0; i < j; i++) {
            c2[i] = _add(c2[i], c2[i+j]);
        }

    {% if BIASED %}
    *(float2*)C_val = _add(c2[0], *(float2*)(bias_share + tx*2));
    {% else %}
    *(float2*)C_val = c2[0];
    {% endif %}

    __syncthreads();
    *(float4*)&fShare[storC + 0*32*8] = *(float4*)regC[4];
    *(float4*)&fShare[storC + 1*32*8] = *(float4*)regC[5];
    *(float4*)&fShare[storC + 2*32*8] = *(float4*)regC[6];
    *(float4*)&fShare[storC + 3*32*8] = *(float4*)regC[7];
    __syncthreads();

    for (int i = 0; i < 8; i++)
        c2[i] = *(float2*)&fShare[readC + i*32];

    // Tree reduce
    for (int j = 4; j > 0; j >>= 1)
        for (int i = 0; i < j; i++) {
            c2[i] = _add(c2[i], c2[i+j]);
        }

    {% if COMPRESSED %}
    C_val += 16 * BLOCK_SIZE_N;
    {% else %}
    C_val += 16 * N;
    {% endif %}
    {% if BIASED %}
    *(float2*)C_val = _add(c2[0], *(float2*)(bias_share + tx*2));
    {% else %}
    *(float2*)C_val = c2[0];
    {% endif %}
}