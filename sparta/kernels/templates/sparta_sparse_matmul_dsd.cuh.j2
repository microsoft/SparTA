{# Copyright (c) Microsoft Corporation. #}
{# Licensed under the MIT license. #}

{% set IDX_T = "long long" if BCSR and BCSC else "int" %}

const int BM = {{ BLOCK_SIZE_M_VALUE }};
const int BK = {{ BLOCK_SIZE_K_VALUE }};
const int BN = {{ BLOCK_SIZE_N_VALUE }};
const int TM = {{ THREAD_SIZE_M_VALUE }};
const int TK = {{ THREAD_SIZE_K_VALUE }};
const int TN = {{ THREAD_SIZE_N_VALUE }};

__global__ void BLOCK_SPARSE_MATMUL(
    float* input_A,
    float* input_B_val,
    {% if BIASED %}float* input_bias,{% endif %}
    float* output_C,
    int* input_B_block_ptr,
    {{ IDX_T }}* input_B_block_idx,
    int B_block_nnz,
    int M,
    int K,
    int N
) {
    float * A = reinterpret_cast<float*>(input_A);
    float * B_val = reinterpret_cast<float*>(input_B_val);
    {% if BIASED %}float * bias = reinterpret_cast<float*>(input_bias);{% endif %}
    float * C = reinterpret_cast<float*>(output_C);
    int * B_block_ptr = reinterpret_cast<int*>(input_B_block_ptr);
    {{ IDX_T }} * B_block_idx = reinterpret_cast<{{ IDX_T }}*>(input_B_block_idx);

    int by = blockIdx.y;
    int bx = blockIdx.x;
    int ty = threadIdx.y;
    int tx = threadIdx.x;

    {% if BATCHED %}
    A += M * K * blockIdx.z;
    {% if COMPRESSED %}
    B_val += B_block_nnz * BN * BK * blockIdx.z;
    {% else %}
    B_val += K * N * blockIdx.z;
    {% endif %}
    C += M * N * blockIdx.z;
    {% if BIASED %}
    bias += N * blockIdx.z;
    {% endif %}
    {% endif %}

    __shared__ float As[BM * BK];
    __shared__ float Bs[BN * BK];

    float accum[TN][TM] = {0};
    float a_frag[TM][TK];
    float b_frag[TN][TK];

    int A_THREAD_PER_ROW = {% if TRANSPOSE_A %}BM{% else %}BK{% endif %} / 4;
    int B_THREAD_PER_ROW = {% if TRANSPOSE_B %}BK{% else %}BN{% endif %} / 4;

    int bszy = BM / TM;
    int bszx = BN / TN;

    int THREADS_PER_BLOCK = bszy * bszx;

    int A_TILE_ROW_STRIDE = THREADS_PER_BLOCK / A_THREAD_PER_ROW;
    int B_TILE_ROW_STRIDE = THREADS_PER_BLOCK / B_THREAD_PER_ROW;

    int tid = ty * bszx + tx;

    int A_BLOCK_ROW_START = tid / A_THREAD_PER_ROW;
    int B_BLOCK_ROW_START = tid / B_THREAD_PER_ROW;

    int A_BLOCK_COL_START = tid % A_THREAD_PER_ROW * 4;
    int B_BLOCK_COL_START = tid % B_THREAD_PER_ROW * 4;

    int index_start = B_block_ptr[bx], index_end = B_block_ptr[bx+1];

    const int vBLOCK_SIZE_M = BM / TM;
    const int vBLOCK_SIZE_N = BN / TN;
    {% if TRANSPOSE_A or TRANSPOSE_B %}float4 tmp_float4;{% endif %}
    for (int tile_block_idx = index_start; tile_block_idx < index_end; tile_block_idx += 1) {
        {{ IDX_T }} idx = B_block_idx[tile_block_idx];
        {% if BCSR and BCSC %}
        int block_idx = idx >> 32;
        {% else %}
        int block_idx = tile_block_idx;
        {% endif %}
        int tile_idx = (idx & 0xffff) * BK;
        #pragma unroll
        for (int k = 0; k < {% if TRANSPOSE_A %}BK{% else %}BM{% endif %}; k += A_TILE_ROW_STRIDE) {
            {% if TRANSPOSE_A %}
            tmp_float4 = (reinterpret_cast<float4*>(&A[(tile_idx + A_BLOCK_ROW_START+k) * M + by*BM + A_BLOCK_COL_START]))[0];
            As[(A_BLOCK_COL_START+0) * BK + k+A_BLOCK_ROW_START] = tmp_float4.x;
            As[(A_BLOCK_COL_START+1) * BK + k+A_BLOCK_ROW_START] = tmp_float4.y;
            As[(A_BLOCK_COL_START+2) * BK + k+A_BLOCK_ROW_START] = tmp_float4.z;
            As[(A_BLOCK_COL_START+3) * BK + k+A_BLOCK_ROW_START] = tmp_float4.w;
            {% else %}
            *((float4 *)(&As[(k+A_BLOCK_ROW_START) * BK + A_BLOCK_COL_START])) =
                *((float4 *)(&A[(by*BM + k+A_BLOCK_ROW_START) * K + tile_idx + A_BLOCK_COL_START]));
            {% endif %}
        }

        #pragma unroll
        for (int k = 0; k < {% if TRANSPOSE_B %}BN{% else %}BK{% endif %}; k += B_TILE_ROW_STRIDE) {
            {% if TRANSPOSE_B %}
                {% if COMPRESSED %}
            tmp_float4 = (reinterpret_cast<float4*>(&B_val[block_idx * BN * BK + (k+B_BLOCK_ROW_START) * BK + B_BLOCK_COL_START]))[0];
                {% else %}
            tmp_float4 = (reinterpret_cast<float4*>(&B_val[(bx*BN + B_BLOCK_ROW_START+k) * K + tile_idx + B_BLOCK_COL_START]))[0];
                {% endif %}
            Bs[(B_BLOCK_COL_START+0) * BN + k+B_BLOCK_ROW_START] = tmp_float4.x;
            Bs[(B_BLOCK_COL_START+1) * BN + k+B_BLOCK_ROW_START] = tmp_float4.y;
            Bs[(B_BLOCK_COL_START+2) * BN + k+B_BLOCK_ROW_START] = tmp_float4.z;
            Bs[(B_BLOCK_COL_START+3) * BN + k+B_BLOCK_ROW_START] = tmp_float4.w;
            {% else %}
            *((float4 *)(&Bs[(k+B_BLOCK_ROW_START) * BN + B_BLOCK_COL_START])) =
                {% if COMPRESSED %}
                *((float4 *)(&B_val[block_idx * BN * BK + (k+B_BLOCK_ROW_START) * BN + B_BLOCK_COL_START]));
                {% else %}
                *((float4 *)(&B_val[(tile_idx + B_BLOCK_ROW_START+k) * N + bx*BN + B_BLOCK_COL_START]));
                {% endif %}
            {% endif %}
        }

        __syncthreads();

        #pragma unroll
        for (int k = 0; k < BK; k += TK) {
            #pragma unroll
            for (int i = 0; i < TK; i++) {
                #pragma unroll
                for (int j = 0; j < TM; j += 1) {
                    a_frag[j][i] = As[(ty + vBLOCK_SIZE_M * j) * BK + k + i];
                }
            }

            #pragma unroll
            for (int i = 0; i < TK; i++) {
                #pragma unroll
                for (int j = 0; j < TN; j += 1) {
                    b_frag[j][i] = Bs[(k + i) * BN + tx + vBLOCK_SIZE_N * j];
                }
            }

            #pragma unroll
            for (int i = 0; i < TN; i++) {
                #pragma unroll
                for (int j = 0; j < TM; j++) {
                    #pragma unroll
                    for (int k_in = 0; k_in < TK; k_in++) {
                        accum[i][j] += a_frag[j][k_in] * b_frag[i][k_in];
                    }
                }
            }
        }

        __syncthreads();
    }

    {% if BIASED %}
    float bias_local[TN];
    for (int thread_x = 0; thread_x < TN; thread_x++) {
        bias_local[thread_x] = bias[BN * bx + tx + thread_x * vBLOCK_SIZE_N];
    }
    {% endif %}

    #pragma unroll
    for (int thread_x = 0; thread_x < TN; thread_x++) {
        #pragma unroll
        for (int thread_y = 0; thread_y < TM; thread_y+=1) {
            C[(BM * by + ty + thread_y * vBLOCK_SIZE_M) * N + BN * bx + tx + thread_x * vBLOCK_SIZE_N] =
                (accum[thread_x][thread_y]){% if BIASED %} + bias_local[thread_x]{% endif %};
        }
    }
}
